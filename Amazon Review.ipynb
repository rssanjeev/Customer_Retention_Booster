{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-339-a4f9b68ac0c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Sanjeev Ramasamy/Downloads/reviews_Electronics_5.json.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-339-a4f9b68ac0c8>\u001b[0m in \u001b[0;36mgetDF\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-339-a4f9b68ac0c8>\u001b[0m in \u001b[0;36mparse\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32myield\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m             \u001b[0muncompress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('C:/Users/Sanjeev Ramasamy/Downloads/reviews_Electronics_5.json.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanjeev Ramasamy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (1,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"D:/Python/1429_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>keys</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>reviews.date</th>\n",
       "      <th>reviews.dateAdded</th>\n",
       "      <th>reviews.dateSeen</th>\n",
       "      <th>...</th>\n",
       "      <th>reviews.doRecommend</th>\n",
       "      <th>reviews.id</th>\n",
       "      <th>reviews.numHelpful</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.sourceURLs</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.userCity</th>\n",
       "      <th>reviews.userProvince</th>\n",
       "      <th>reviews.username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>Kindle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>very fast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>Beginner tablet for our 9 year old son.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DaveZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>Good!!!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-12T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>Fantastic Tablet for kids</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explore42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               name  \\\n",
       "0  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "1  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "2  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "3  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "4  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "\n",
       "        asins   brand                                         categories  \\\n",
       "0  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "1  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "2  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "3  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "4  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "\n",
       "                                                keys manufacturer  \\\n",
       "0  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "1  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "2  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "3  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "4  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "\n",
       "               reviews.date     reviews.dateAdded  \\\n",
       "0  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "1  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "2  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "3  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "4  2017-01-12T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "\n",
       "                                    reviews.dateSeen        ...         \\\n",
       "0  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z        ...          \n",
       "1  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z        ...          \n",
       "2  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z        ...          \n",
       "3  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z        ...          \n",
       "4  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z        ...          \n",
       "\n",
       "  reviews.doRecommend reviews.id  reviews.numHelpful  reviews.rating  \\\n",
       "0                True        NaN                 0.0             5.0   \n",
       "1                True        NaN                 0.0             5.0   \n",
       "2                True        NaN                 0.0             5.0   \n",
       "3                True        NaN                 0.0             4.0   \n",
       "4                True        NaN                 0.0             5.0   \n",
       "\n",
       "                                  reviews.sourceURLs  \\\n",
       "0  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "1  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "2  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "3  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "4  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  This product so far has not disappointed. My c...   \n",
       "1  great for beginner or experienced person. Boug...   \n",
       "2  Inexpensive tablet for him to use and learn on...   \n",
       "3  I've had my Fire HD 8 two weeks now and I love...   \n",
       "4  I bought this for my grand daughter when she c...   \n",
       "\n",
       "                             reviews.title reviews.userCity  \\\n",
       "0                                   Kindle              NaN   \n",
       "1                                very fast              NaN   \n",
       "2  Beginner tablet for our 9 year old son.              NaN   \n",
       "3                                  Good!!!              NaN   \n",
       "4                Fantastic Tablet for kids              NaN   \n",
       "\n",
       "   reviews.userProvince  reviews.username  \n",
       "0                   NaN           Adapter  \n",
       "1                   NaN            truman  \n",
       "2                   NaN             DaveZ  \n",
       "3                   NaN            Shacks  \n",
       "4                   NaN         explore42  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df[['reviews.text','reviews.rating']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating\n",
       "0  This product so far has not disappointed. My c...     5.0\n",
       "1  great for beginner or experienced person. Boug...     5.0\n",
       "2  Inexpensive tablet for him to use and learn on...     5.0\n",
       "3  I've had my Fire HD 8 two weeks now and I love...     4.0\n",
       "4  I bought this for my grand daughter when she c...     5.0"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.columns = [\"reviews\", \"rating\"]\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['Sentiment'] = 1\n",
    "text.loc[(text['rating'] > 0) & (text['rating'] <= 2), 'Sentiment'] =0\n",
    "text.loc[(text['rating'] > 3) & (text['rating'] <= 5), 'Sentiment'] = 2\n",
    "text=text[['reviews','Sentiment']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.columns = [\"reviews\",\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=text['Sentiment'].values\n",
    "#X=text['reviews.text'].values\n",
    "X = text.reviews.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20796,) (20796,) (13864,) (13864,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "#print(X_train[0])\n",
    "#print(y_train[0])\n",
    "#print(X_test[0])\n",
    "#print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n",
      "[[    0   502]\n",
      " [    1   954]\n",
      " [    2 19340]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanjeev Ramasamy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: `itemfreq` is deprecated!\n",
      "`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "training_labels = set(y_train)\n",
    "print(training_labels)\n",
    "from scipy.stats import itemfreq\n",
    "training_category_dist = itemfreq(y_train)\n",
    "print(training_category_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  unigram boolean vectorizer, set minimum document frequency to 5\n",
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram tfidf vectorizer, set minimum document frequency to 5\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec1 = unigram_bool_vectorizer.fit_transform(X_train)\n",
    "X_train_vec2 = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_train_vec3 = unigram_tfidf_vectorizer.fit_transform(X_train)\n",
    "X_train_vec4 = gram12_count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the test dataset\n",
    "X_test_vec1 = unigram_bool_vectorizer.transform(X_test)\n",
    "X_test_vec2 = unigram_count_vectorizer.transform(X_test)\n",
    "X_test_vec3 = unigram_tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec4 = gram12_count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the MNB module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_clf = LinearSVC(C=1)\n",
    "# initialize the MNB model\n",
    "nb_clf= MultinomialNB()\n",
    "\n",
    "# use the training data to train the MNB model\n",
    "#svm_clf.fit(X_train_vec,y_train)\n",
    "#nb_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_BOOL Score: 0.9262839007501442\n"
     ]
    }
   ],
   "source": [
    "svm_model1 = svm_clf.fit(X_train_vec1,y_train)\n",
    "svm_bool = svm_model1.score(X_test_vec1,y_test)\n",
    "print(\"SVM_BOOL Score:\",svm_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(1.1700572596625316, 'refused')\n",
      "(1.175495162170946, 'leather')\n",
      "(1.1755297258835078, 'overdrive')\n",
      "(1.1880102199258604, 'damaged')\n",
      "(1.2141045747857873, 'terrible')\n",
      "(1.2771417922977548, 'waste')\n",
      "(1.3653509814523388, 'dust')\n",
      "(1.3892617466152772, 'accessibility')\n",
      "(1.4323304894568045, 'advice')\n",
      "(1.5066043807121505, 'comfortably')\n",
      "\n",
      "not very negative words\n",
      "(-1.54727473232284, 'allowed')\n",
      "(-1.3575625509221558, 'desk')\n",
      "(-1.1800077403759661, 'break')\n",
      "(-1.1090125438950047, 'lady')\n",
      "(-1.1077445589045367, 'instagram')\n",
      "(-1.106019640740882, 'laptop')\n",
      "(-1.0863756286581552, 'browser')\n",
      "(-1.0693418758773088, 'hd')\n",
      "(-1.0434765824916354, 'frozen')\n",
      "(-1.0214474014033168, 'stick')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_feature_ranks = sorted(zip(svm_model1.coef_[0], unigram_bool_vectorizer.get_feature_names()))\n",
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "very_negative_10 = svm_feature_ranks[-10:]\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = svm_feature_ranks[:10]\n",
    "print(\"not very negative words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   75    55   180]\n",
      " [   43    89   446]\n",
      " [   76   222 12678]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.24      0.30       310\n",
      "           1       0.24      0.15      0.19       578\n",
      "           2       0.95      0.98      0.96     12976\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     13864\n",
      "   macro avg       0.53      0.46      0.48     13864\n",
      "weighted avg       0.91      0.93      0.92     13864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix and classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = svm_model1.predict(X_test_vec1)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "target_names = ['0','1','2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCM_COUNT_VECT Score: 0.9737930371225235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanjeev Ramasamy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svm_model2 = svm_clf.fit(X_train_vec2,y_train)\n",
    "svm_count = svm_model2.score(X_train_vec2,y_train)\n",
    "print(\"SCM_COUNT_VECT Score:\",svm_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(1.1378535600726738, 'bloatware')\n",
      "(1.1438991800393812, 'leather')\n",
      "(1.1514338066669312, 'barely')\n",
      "(1.1718219565978045, 'waste')\n",
      "(1.2333940364016094, 'terrible')\n",
      "(1.3352455457602035, 'damaged')\n",
      "(1.4022013003722333, 'comfortably')\n",
      "(1.4467378584040955, 'dust')\n",
      "(1.4703316073138037, 'advice')\n",
      "(1.5017629006311344, 'accessibility')\n",
      "\n",
      "not very negative words\n",
      "(-1.6055630125914653, 'allowed')\n",
      "(-1.1610864280028337, 'excellent')\n",
      "(-1.1403708203342486, 'desk')\n",
      "(-1.0941849606197678, 'overall')\n",
      "(-1.0716095686681923, 'browser')\n",
      "(-1.0682998587956094, 'dropping')\n",
      "(-1.0479614870696425, 'frozen')\n",
      "(-1.044033108234935, 'assistance')\n",
      "(-1.0414857544717213, 'laptop')\n",
      "(-1.0410931409869446, 'break')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_feature_ranks = sorted(zip(svm_model2.coef_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "very_negative_10 = svm_feature_ranks[-10:]\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = svm_feature_ranks[:10]\n",
    "print(\"not very negative words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   77    55   178]\n",
      " [   46    89   443]\n",
      " [   91   223 12662]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.25      0.29       310\n",
      "           1       0.24      0.15      0.19       578\n",
      "           2       0.95      0.98      0.96     12976\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     13864\n",
      "   macro avg       0.52      0.46      0.48     13864\n",
      "weighted avg       0.91      0.93      0.92     13864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model2.predict(X_test_vec2)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "\n",
    "target_names = ['0','1','2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_TFIDF_Score: 0.9367426428159261\n"
     ]
    }
   ],
   "source": [
    "svm_model3 = svm_clf.fit(X_train_vec3,y_train)\n",
    "svm_tfidf = svm_model3.score(X_test_vec3,y_test)\n",
    "print(\"SVM_TFIDF_Score:\", svm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(1.5829769878140716, 'refurbished')\n",
      "(1.5985953070142824, 'poor')\n",
      "(1.6046804237275036, 'disappointed')\n",
      "(1.617294077880973, 'hopes')\n",
      "(1.625479761163857, 'returned')\n",
      "(1.671776065749695, 'returning')\n",
      "(1.7842633804616337, 'waste')\n",
      "(1.7844531319174677, 'barely')\n",
      "(1.7947553651934098, 'useless')\n",
      "(2.0308106455033434, 'terrible')\n",
      "\n",
      "not very negative words\n",
      "(-1.6723727912898299, 'great')\n",
      "(-1.579797499129392, 'stick')\n",
      "(-1.4416254668080462, 'overall')\n",
      "(-1.3982451750726355, 'easy')\n",
      "(-1.3097029962977214, 'excellent')\n",
      "(-1.2958161166021203, 'hd')\n",
      "(-1.2477508905874737, 'love')\n",
      "(-1.2150916885180088, 'reading')\n",
      "(-1.125124532893256, 'laptop')\n",
      "(-1.003718077415268, 'pleased')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_feature_ranks = sorted(zip(svm_model3.coef_[0], unigram_tfidf_vectorizer.get_feature_names()))\n",
    "very_negative_10 = svm_feature_ranks[-10:]\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = svm_feature_ranks[:10]\n",
    "print(\"not very negative words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   58    36   216]\n",
      " [   28    37   513]\n",
      " [   29    55 12892]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.19      0.27       310\n",
      "           1       0.29      0.06      0.10       578\n",
      "           2       0.95      0.99      0.97     12976\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     13864\n",
      "   macro avg       0.58      0.41      0.45     13864\n",
      "weighted avg       0.91      0.94      0.92     13864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model3.predict(X_test_vec3)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "\n",
    "target_names = ['0','1','2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_NGRAM Score: 0.9199365262550491\n"
     ]
    }
   ],
   "source": [
    "svm_model4 = svm_clf.fit(X_train_vec4,y_train)\n",
    "svm_ngram = svm_model4.score(X_test_vec4, y_test)\n",
    "print(\"SVM_NGRAM Score:\",svm_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(1.012894273121367, 'bloatware')\n",
      "(1.0151058445274097, 'want buy')\n",
      "(1.0221574273134681, 'damaged')\n",
      "(1.023874148364283, 'advice')\n",
      "(1.035198111264926, 'comfortably')\n",
      "(1.0686129980268941, 'price awesome')\n",
      "(1.0838661158830638, 'night light')\n",
      "(1.0905801865357994, 'great year')\n",
      "(1.21552454040574, 'kindle perfect')\n",
      "(1.4207349440993882, 'need help')\n",
      "\n",
      "not very negative words\n",
      "(-0.9021454156852892, 'frozen')\n",
      "(-0.8547670083067317, 'laptop')\n",
      "(-0.8449513343801914, 'overall')\n",
      "(-0.8436830221303548, 'stick')\n",
      "(-0.8411691554254214, 'holding')\n",
      "(-0.8141050047739389, 'excellent')\n",
      "(-0.7973407697590842, 'amazon echo')\n",
      "(-0.7795168222260991, 'easy')\n",
      "(-0.7668394727424561, 'great product')\n",
      "(-0.7584938187197625, 'didn need')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_feature_ranks = sorted(zip(svm_model4.coef_[0], gram12_count_vectorizer.get_feature_names()))\n",
    "very_negative_10 = svm_feature_ranks[-10:]\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = svm_feature_ranks[:10]\n",
    "print(\"not very negative words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   83    61   166]\n",
      " [   49    91   438]\n",
      " [   91   305 12580]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.27      0.31       310\n",
      "           1       0.20      0.16      0.18       578\n",
      "           2       0.95      0.97      0.96     12976\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     13864\n",
      "   macro avg       0.51      0.46      0.48     13864\n",
      "weighted avg       0.91      0.92      0.91     13864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model4.predict(X_test_vec4)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "print(cm)\n",
    "print()\n",
    "target_names = ['0','1','2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_Boolean Score: 0.9218118869013272\n"
     ]
    }
   ],
   "source": [
    "nb_model1 = nb_clf.fit(X_train_vec1,y_train)\n",
    "nb_bool = nb_model1.score(X_test_vec1,y_test)\n",
    "print(\"NB_Boolean Score:\",nb_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(-5.233864863216204, 'product')\n",
      "(-5.17134450623487, 'good')\n",
      "(-5.17134450623487, 'like')\n",
      "(-5.141491543085189, 'kindle')\n",
      "(-5.112504006211936, 'buy')\n",
      "(-5.09831937121998, 'use')\n",
      "(-5.056934155057125, 'just')\n",
      "(-4.954280000997042, 'bought')\n",
      "(-4.633611429548955, 'tablet')\n",
      "(-4.448344362525243, 'amazon')\n",
      "\n",
      "not very negative words\n",
      "(-9.360999248261296, '10x')\n",
      "(-9.360999248261296, '128')\n",
      "(-9.360999248261296, '128gb')\n",
      "(-9.360999248261296, '150')\n",
      "(-9.360999248261296, '16g')\n",
      "(-9.360999248261296, '16gb')\n",
      "(-9.360999248261296, '17')\n",
      "(-9.360999248261296, '18')\n",
      "(-9.360999248261296, '2011')\n",
      "(-9.360999248261296, '2012')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_feature_ranks = sorted(zip(nb_model1.coef_[0], unigram_bool_vectorizer.get_feature_names()))\n",
    "very_negative_10 = nb_feature_ranks[-10:]\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = nb_feature_ranks[:10]\n",
    "print(\"not very negative words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   87    73   150]\n",
      " [   40   118   420]\n",
      " [  107   294 12575]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.28      0.32       310\n",
      "           1       0.24      0.20      0.22       578\n",
      "           2       0.96      0.97      0.96     12976\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     13864\n",
      "   macro avg       0.52      0.48      0.50     13864\n",
      "weighted avg       0.91      0.92      0.92     13864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_model1.predict(X_test_vec1)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "print(cm)\n",
    "print()\n",
    "target_names = ['0','1','2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_Count Score: 0.9214512406231967\n"
     ]
    }
   ],
   "source": [
    "nb_model2 = nb_clf.fit(X_train_vec2,y_train)\n",
    "nb_count = nb_model2.score(X_test_vec2,y_test)\n",
    "print(\"NB_Count Score:\",nb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(-5.081640204771728, 'device')\n",
      "(-5.069061422564868, 'apps')\n",
      "(-5.020271258395436, 'like')\n",
      "(-4.9851799385841655, 'use')\n",
      "(-4.973751242760542, 'buy')\n",
      "(-4.96245168750661, 'just')\n",
      "(-4.9402285507218995, 'bought')\n",
      "(-4.816359069009113, 'kindle')\n",
      "(-4.357337856431987, 'tablet')\n",
      "(-4.209341042179107, 'amazon')\n",
      "\n",
      "not very negative words\n",
      "(-9.45108805723875, '10x')\n",
      "(-9.45108805723875, '128')\n",
      "(-9.45108805723875, '128gb')\n",
      "(-9.45108805723875, '150')\n",
      "(-9.45108805723875, '16g')\n",
      "(-9.45108805723875, '16gb')\n",
      "(-9.45108805723875, '17')\n",
      "(-9.45108805723875, '18')\n",
      "(-9.45108805723875, '2011')\n",
      "(-9.45108805723875, '2012')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_feature_ranks = sorted(zip(nb_model2.coef_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "very_negative_10 = nb_feature_ranks[-10:]\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = nb_feature_ranks[:10]\n",
    "print(\"not very negative words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   99    74   137]\n",
      " [   48   123   407]\n",
      " [  101   322 12553]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.32      0.35       310\n",
      "           1       0.24      0.21      0.22       578\n",
      "           2       0.96      0.97      0.96     12976\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     13864\n",
      "   macro avg       0.53      0.50      0.51     13864\n",
      "weighted avg       0.92      0.92      0.92     13864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_model2.predict(X_test_vec2)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "print(cm)\n",
    "print()\n",
    "target_names = ['0','1','2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_TFIDF SCore: 0.9363819965377957\n"
     ]
    }
   ],
   "source": [
    "nb_model3 = nb_clf.fit(X_train_vec3,y_train)\n",
    "nb_tfdf = nb_model3.score(X_test_vec3,y_test)\n",
    "print(\"NB_TFIDF SCore:\",nb_tfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(-6.080778852486594, 'bought')\n",
      "(-6.055817559456606, 'just')\n",
      "(-6.034696873043576, 'apps')\n",
      "(-6.033012129094146, 'kindle')\n",
      "(-5.960356734657003, 'don')\n",
      "(-5.959442485797114, 'buy')\n",
      "(-5.95292311397702, 'slow')\n",
      "(-5.933110089174585, 'returned')\n",
      "(-5.691932322822391, 'tablet')\n",
      "(-5.547766948861756, 'amazon')\n",
      "\n",
      "not very negative words\n",
      "(-8.547880916516542, '10x')\n",
      "(-8.547880916516542, '128')\n",
      "(-8.547880916516542, '128gb')\n",
      "(-8.547880916516542, '150')\n",
      "(-8.547880916516542, '16g')\n",
      "(-8.547880916516542, '16gb')\n",
      "(-8.547880916516542, '17')\n",
      "(-8.547880916516542, '18')\n",
      "(-8.547880916516542, '2011')\n",
      "(-8.547880916516542, '2012')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_feature_ranks = sorted(zip(nb_model3.coef_[0], unigram_tfidf_vectorizer.get_feature_names()))\n",
    "very_negative_10 = nb_feature_ranks[-10:]\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = nb_feature_ranks[:10]\n",
    "print(\"not very negative words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    4     2   304]\n",
      " [    1     3   574]\n",
      " [    0     1 12975]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.01      0.03       310\n",
      "           1       0.50      0.01      0.01       578\n",
      "           2       0.94      1.00      0.97     12976\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     13864\n",
      "   macro avg       0.75      0.34      0.33     13864\n",
      "weighted avg       0.92      0.94      0.91     13864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_model3.predict(X_test_vec3)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "print(cm)\n",
    "print()\n",
    "target_names = ['0','1','2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_NGRAM Score: 0.9241921523369879\n"
     ]
    }
   ],
   "source": [
    "nb_model4 = nb_clf.fit(X_train_vec4,y_train)\n",
    "nb_ngram = nb_model4.score(X_test_vec4,y_test)\n",
    "print(\"NB_NGRAM Score:\",nb_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(-5.618518706647046, 'device')\n",
      "(-5.605939924440186, 'apps')\n",
      "(-5.557149760270754, 'like')\n",
      "(-5.5220584404594835, 'use')\n",
      "(-5.51062974463586, 'buy')\n",
      "(-5.4993301893819275, 'just')\n",
      "(-5.477107052597217, 'bought')\n",
      "(-5.353237570884431, 'kindle')\n",
      "(-4.894216358307305, 'tablet')\n",
      "(-4.746219544054425, 'amazon')\n",
      "\n",
      "not very negative words\n",
      "(-9.987966559114067, '10 inch')\n",
      "(-9.987966559114067, '10 love')\n",
      "(-9.987966559114067, '10 minutes')\n",
      "(-9.987966559114067, '10 year')\n",
      "(-9.987966559114067, '10 years')\n",
      "(-9.987966559114067, '10 yr')\n",
      "(-9.987966559114067, '100 dollars')\n",
      "(-9.987966559114067, '100 month')\n",
      "(-9.987966559114067, '10x')\n",
      "(-9.987966559114067, '11 year')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_feature_ranks = sorted(zip(nb_model4.coef_[0], gram12_count_vectorizer.get_feature_names()))\n",
    "very_negative_10 = nb_feature_ranks[-10:]\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = nb_feature_ranks[:10]\n",
    "print(\"not very negative words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   72    91   147]\n",
      " [   25   150   403]\n",
      " [   63   322 12591]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.23      0.31       310\n",
      "           1       0.27      0.26      0.26       578\n",
      "           2       0.96      0.97      0.96     12976\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     13864\n",
      "   macro avg       0.56      0.49      0.51     13864\n",
      "weighted avg       0.92      0.92      0.92     13864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_model4.predict(X_test_vec4)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "print(cm)\n",
    "print()\n",
    "target_names = ['0','1','2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCM_COUNT_VECT Score: 0.9737930371225235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanjeev Ramasamy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svm_model2 = svm_clf.fit(X_train_vec2,y_train)\n",
    "svm_count = svm_model2.score(X_train_vec2,y_train)\n",
    "print(\"SCM_COUNT_VECT Score:\",svm_count)\n",
    "y_pred = svm_model2.predict(X_test_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.DataFrame()\n",
    "output[\"reviews\"]=list(X_test)\n",
    "output[\"Prediction\"]=y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's very durable and Nice picture.its the per...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fire TV helps me navigate several viewing sour...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Touch is not good. Also very limited apps on a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is one cool item. It doesn't take up a lo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It's amazing and I will like to tell a friend ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amazon has hit it out of the park with this on...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Perfect device for kids learning how to use to...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I bought this thinking it was needed for the E...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Makes lists for me, plays music, tells me the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pretty good sound for the size and configurati...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>perfect size; good graphics; app store is exte...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Great product. I have two of these and a coupl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Absolutely makes my home feel more updated. I ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Absolutely can't get enough of my fire TV. Bes...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I bought this for my sister as a birthday gift...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I am just learning all the possible functions ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>not as smart as the commercials show. cant ans...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Great video quality lots of fun apps fun for t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The tablet could be great for my 4 year old da...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reviews  Prediction\n",
       "1   It's very durable and Nice picture.its the per...           2\n",
       "2   Fire TV helps me navigate several viewing sour...           2\n",
       "3   Touch is not good. Also very limited apps on a...           2\n",
       "4   This is one cool item. It doesn't take up a lo...           2\n",
       "5   It's amazing and I will like to tell a friend ...           2\n",
       "6   Amazon has hit it out of the park with this on...           2\n",
       "7   Perfect device for kids learning how to use to...           2\n",
       "8   I bought this thinking it was needed for the E...           2\n",
       "9   Makes lists for me, plays music, tells me the ...           2\n",
       "10  Pretty good sound for the size and configurati...           2\n",
       "11  perfect size; good graphics; app store is exte...           2\n",
       "12  Great product. I have two of these and a coupl...           2\n",
       "13  Absolutely makes my home feel more updated. I ...           2\n",
       "14  Absolutely can't get enough of my fire TV. Bes...           2\n",
       "15  I bought this for my sister as a birthday gift...           2\n",
       "16  I am just learning all the possible functions ...           2\n",
       "17  not as smart as the commercials show. cant ans...           2\n",
       "18  Great video quality lots of fun apps fun for t...           2\n",
       "19  The tablet could be great for my 4 year old da...           0"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing2=output.loc[output['Prediction'] == 2]\n",
    "Testing1=output.loc[output['Prediction'] == 1]\n",
    "Testing0=output.loc[output['Prediction'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13283\n",
      "367\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "print(len(Testing2))\n",
    "print(len(Testing1))\n",
    "print(len(Testing0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon - Kindle Paperwhite is moe like reading...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's very durable and Nice picture.its the per...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fire TV helps me navigate several viewing sour...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Touch is not good. Also very limited apps on a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is one cool item. It doesn't take up a lo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  Prediction\n",
       "0  Amazon - Kindle Paperwhite is moe like reading...           2\n",
       "1  It's very durable and Nice picture.its the per...           2\n",
       "2  Fire TV helps me navigate several viewing sour...           2\n",
       "3  Touch is not good. Also very limited apps on a...           2\n",
       "4  This is one cool item. It doesn't take up a lo...           2"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testing2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tf_vectorizer.fit_transform(list(Testing2[\"reviews\"]))\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 20\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "lda_z = lda.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "time play questions child plays music tell ask favorite keeps\n",
      "Topic 1:\n",
      "best buy device far money amazon price ipad search apple\n",
      "Topic 2:\n",
      "amazon prime app store enjoy apps free google available android\n",
      "Topic 3:\n",
      "worth definitely big upgrade expected better hard hands option exactly\n",
      "Topic 4:\n",
      "purchased recommend son highly friends husband bought recommended gift likes\n",
      "Topic 5:\n",
      "kindle battery new life long especially time charge love great\n",
      "Topic 6:\n",
      "better fast screen picture clear camera memory size faster purse\n",
      "Topic 7:\n",
      "games kids tablet great use easy apps play love user\n",
      "Topic 8:\n",
      "echo alexa music home love use great amazon speaker voice\n",
      "Topic 9:\n",
      "needed buying went started school taking purpose loaded ebooks hooked\n",
      "Topic 10:\n",
      "just don like getting little did used right thing know\n",
      "Topic 11:\n",
      "tv amazon watch movies box stick great streaming use love\n",
      "Topic 12:\n",
      "love wife bought item years absolutely things purchased christmas friend\n",
      "Topic 13:\n",
      "cable ve wish time decided pay high month extra instead\n",
      "Topic 14:\n",
      "kindle read reading books light easy screen love paperwhite like\n",
      "Topic 15:\n",
      "bought loves old gift year perfect tablet daughter christmas happy\n",
      "Topic 16:\n",
      "easy use great tablet purchase work awesome set price simple\n",
      "Topic 17:\n",
      "tablet price great does good nice internet need wanted screen\n",
      "Topic 18:\n",
      "great product good works price quality excellent recommend sound amazing\n",
      "Topic 19:\n",
      "fun small family portable wonderful learn answer order fantastic lot\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tf_vectorizer.fit_transform(list(Testing1[\"reviews\"]))\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 20\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "lda_z = lda.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "good tablet basic use complaint far don close owned explain\n",
      "Topic 1:\n",
      "tablet good amazon price just bought use great like apps\n",
      "Topic 2:\n",
      "4k like apple tv small speed older purchased work version\n",
      "Topic 3:\n",
      "tablet hard books bought perfectly version reading quality amazon size\n",
      "Topic 4:\n",
      "say make port send able attempted charging thought calls care\n",
      "Topic 5:\n",
      "yr worthy wireless day experience dont games mini went major\n",
      "Topic 6:\n",
      "doesn work works phone screen does good pretty properly amazon\n",
      "Topic 7:\n",
      "tablet things amazon pay slow charger price sure kindle did\n",
      "Topic 8:\n",
      "ok tv members controlled apparently speaker thought stock learn pc\n",
      "Topic 9:\n",
      "kids purchases product account tablets great accounts did amazon using\n",
      "Topic 10:\n",
      "larger dot tablet speaker nice speakers external use games leave\n",
      "Topic 11:\n",
      "screen lot order slow great does tablet needs detergent alexa\n",
      "Topic 12:\n",
      "work sad open tab greatest fi minimal android standby wi\n",
      "Topic 13:\n",
      "satisfy bit program know option coming amazons fee child tablet\n",
      "Topic 14:\n",
      "liked games children hoping couldn site locked cons making came\n",
      "Topic 15:\n",
      "charger kindle firetv plug fast does message watch little slow\n",
      "Topic 16:\n",
      "kindle read page reading like books just book case time\n",
      "Topic 17:\n",
      "integrated tap echo ai designed automation products pretty vocabulary built\n",
      "Topic 18:\n",
      "time charging speeds afraid slow im don processing lag tablets\n",
      "Topic 19:\n",
      "home google better returning product ended like charging alexa item\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tf_vectorizer.fit_transform(list(Testing0[\"reviews\"]))\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "no_topics = 20\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "lda_z = lda.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "processor 4k money stock got speed told faster went second\n",
      "Topic 1:\n",
      "amazon books device read membership account free calls wasn setup\n",
      "Topic 2:\n",
      "alexa questions review love work maybe just grandchildren guess young\n",
      "Topic 3:\n",
      "kindle amazon charge just like read page bought don oasis\n",
      "Topic 4:\n",
      "disappointed use doesn mode screen display apps price work daughter\n",
      "Topic 5:\n",
      "news home kind going review basic did webroot place ask\n",
      "Topic 6:\n",
      "kindle time wifi just amazon problem tried got connect product\n",
      "Topic 7:\n",
      "google like won amazon android think buy got youtube best\n",
      "Topic 8:\n",
      "instructions came does returned worth different searching gave using frustrating\n",
      "Topic 9:\n",
      "says alexa connection later getting want sucks check minutes works\n",
      "Topic 10:\n",
      "touch page pdf kindle doesn paper book liked font turning\n",
      "Topic 11:\n",
      "kept customer working service friendly type couldn recommend took person\n",
      "Topic 12:\n",
      "charge definitely won recommend kindles replacement charging problem does 10\n",
      "Topic 13:\n",
      "internet didn pictures tv pixel tablet model wasn unless lot\n",
      "Topic 14:\n",
      "didn don order membership 99 ask answer need know returned\n",
      "Topic 15:\n",
      "wifi time poor returned reader clearly won finally couldn box\n",
      "Topic 16:\n",
      "echo tap dot hoped perfectly excited home make listening works\n",
      "Topic 17:\n",
      "tablet use amazon bought screen don just like apps price\n",
      "Topic 18:\n",
      "unlike version need plugged fully does making great button portable\n",
      "Topic 19:\n",
      "new high complicated went friendly hopes review apple just box\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
